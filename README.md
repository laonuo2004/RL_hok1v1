# RL_hok1v1

北京理工大学 23 级计科大三强化学习刘驰班腾讯开悟平台"智能体决策1V1"实验

## 结论

改代码不如堆时长，没想到 rk1 的模型就是原始代码训练 100 小时，也是幽默完了😅

当然事实证明训练时间也并非越长越好，像是原始代码训练了 120 小时的模型就成了路边一条，总之十分玄学。

不管怎么说，实验一开放就立刻开始训练腾讯提供的 PPO 算法准没错，等到后面代码看懂了、算法改完了再想开始训练，肯定来不及，毕竟能训 100 小时的前提就是要跑满 100 小时，怎么的也得马不停蹄地跑个四五天，而实验本身就一两周时间，前一周还要做峡谷漫步，后一周还要做重返秘境，总体上非常紧张。

## 我们组的情况

我们组最终的战绩是50胜8负，凭借 86.2% 的胜率拿了第二，距离第一 (52胜6负) 差了两局，而且刚好我们和 rk1 打的两场都输了，如果能打平或赢两局，那就能拿第一了。

![战绩](./images/战绩.png)

**训练方法**：

1. 原始代码训练了 80 个小时
2. 随后强化推塔奖励 (差不多就是当前的 main 分支代码) 又训练了 30 个小时，因为我们发现智能体的推塔积极性比较低，很多时候都是站在角落刷兵线，于是做此修改

虽然组内也做了很多修改的尝试，但是发现甚至还打不过原始 Baseline，遂放弃，这里也不过多赘述。总之需要强调的一点是，想要做出真正有效果的修改不是一件容易的事情，不如研究一下训练的策略，例如调参、课程学习等等，让模型训练得更快更好。

不过没想到训了 110 小时还打不过 rk1 的原始代码训练 100 小时，感觉还是挺玄学的，只能说整个实验挺看运气。

## 将本地代码同步到开悟平台上

1. 使用 [pack_and_encode.sh](pack_and_encode.sh) 脚本本地项目打包编码，可能要多执行几次才能成功：
   ```bash
   sh ./pack_and_encode.sh /path/to/your/project/

   ```
2. 开悟平台新建文件 `encoded_output.txt`，将编码后的内容粘贴进去。同时新建脚本文件 `decode_and_unpack.sh`，将 [decode_and_unpack.sh](decode_and_unpack.sh) 复制进去。
3. 随后复制编码到代码实际存放位置当中去，并解码：
   ```bash
   cp ./encoded_output.txt /workspace/code/
   sh ./decode_and_unpack.sh /workspace/code/
   
   ```

同步完成。
